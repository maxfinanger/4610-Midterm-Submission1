{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d461119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np \n",
    "\n",
    "amountOfLocations = None\n",
    "locations = []\n",
    "cars = None\n",
    "depot = None\n",
    "\n",
    "def generate_locations(n, bounds=(0,100), seed=None):\n",
    "    if seed is not None:\n",
    "        rnd = random.Random(seed)\n",
    "        return [(rnd.randint(bounds[0], bounds[1]), rnd.randint(bounds[0], bounds[1])) for _ in range(n)]\n",
    "    return [(random.randint(bounds[0], bounds[1]), random.randint(bounds[0], bounds[1])) for _ in range(n)]\n",
    "\n",
    "def make_scenarios(seed=None):\n",
    "    s = 0 if seed is None else seed\n",
    "    return [\n",
    "        {\"id\":\"small-1\", \"num_vehicles\":3,  \"num_customers\":12, \"customers\": generate_locations(12, seed=s+1)},\n",
    "        {\"id\":\"small-2\", \"num_vehicles\":7,  \"num_customers\":18, \"customers\": generate_locations(18, seed=s+2)},\n",
    "        {\"id\":\"medium-1\",\"num_vehicles\":11, \"num_customers\":20, \"customers\": generate_locations(20, seed=s+3)},\n",
    "        {\"id\":\"medium-2\",\"num_vehicles\":15, \"num_customers\":28, \"customers\": generate_locations(28, seed=s+4)},\n",
    "        {\"id\":\"large-1\", \"num_vehicles\":26, \"num_customers\":30, \"customers\": generate_locations(30, seed=s+5)},\n",
    "        {\"id\":\"large-2\", \"num_vehicles\":30, \"num_customers\":40, \"customers\": generate_locations(40, seed=s+6)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164cb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5\n",
    "\n",
    "def applay_scenario(scenario_configuration):\n",
    "    \"\"\"\n",
    "    FIX: Setup scenario with normalization baseline computation\n",
    "    Establishes naive baseline for fair cross-scenario fitness comparison\n",
    "    \"\"\"\n",
    "    global amountOfLocations, locations, cars, depot, dist_matrix, dist_to_depot, naive_baseline_cost\n",
    "    \n",
    "    amountOfLocations = scenario_configuration[\"num_customers\"]\n",
    "    locations = scenario_configuration[\"customers\"][:]\n",
    "    cars = scenario_configuration[\"num_vehicles\"]\n",
    "\n",
    "    depot = scenario_configuration.get(\"depot\", (50,50))\n",
    "    \n",
    "    # Precompute distance matrices for computational efficiency\n",
    "    num_customer_locations = len(locations)\n",
    "    dist_matrix = np.zeros((num_customer_locations, num_customer_locations))\n",
    "    for customer_i in range(num_customer_locations):\n",
    "        for customer_j in range(num_customer_locations):\n",
    "            if customer_i != customer_j:\n",
    "                dist_matrix[customer_i][customer_j] = distance(locations[customer_i], locations[customer_j])\n",
    "    \n",
    "    dist_to_depot = np.array([distance(depot, locations[customer_idx]) \n",
    "                             for customer_idx in range(num_customer_locations)])\n",
    "    naive_baseline_cost = float(2.0 * np.sum(dist_to_depot))\n",
    "\n",
    "def compute_normalized_solution_quality(actual_total_distance):\n",
    "    \"\"\"\"\n",
    "    Compute normalized fitness a primary solution quality metric\n",
    "    Higher values indicate better solution quality realative to simple basline. \n",
    "    \"\"\"\n",
    "    if actual_total_distance == float('inf') or actual_total_distance <= 0:\n",
    "        return 0.0  \n",
    "    \n",
    "    \n",
    "    basline_cost = globals().get('naive_baseline_cost', None)\n",
    "    if basline_cost is None or basline_cost  <= 0.0: \n",
    "        \n",
    "        return 1.0/ (1.0+ actual_total_distance)\n",
    "    \n",
    "    normalized_quality_score = float(basline_cost/actual_total_distance)\n",
    "    return normalized_quality_score\n",
    "\n",
    "\n",
    "def get_vrp_cost_and_routes(individual_permutation, num_available_vehicles):\n",
    "    num_customers = len(individual_permutation)\n",
    "    if num_customers == 0:\n",
    "        return float('inf'), []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        customer_to_customer_distances = dist_matrix\n",
    "        depot_to_customer_distances = dist_to_depot\n",
    "        if num_customers >= 2:\n",
    "            consecutive_edge_distances = [customer_to_customer_distances[individual_permutation[i]][individual_permutation[i+1]] \n",
    "                                        for i in range(num_customers-1)]\n",
    "        else:\n",
    "            consecutive_edge_distances = []  \n",
    "    except NameError:\n",
    "        if num_customers >= 2:\n",
    "            consecutive_edge_distances = [distance(locations[individual_permutation[i]], \n",
    "                                                 locations[individual_permutation[i+1]]) \n",
    "                                        for i in range(num_customers-1)]\n",
    "        else:\n",
    "            consecutive_edge_distances = []\n",
    "        depot_to_customer_distances = None\n",
    "    \n",
    "    \n",
    "    prefix_distance_sums = np.zeros(num_customers)\n",
    "    if num_customers >= 2:\n",
    "        prefix_distance_sums[1:] = np.cumsum(consecutive_edge_distances)\n",
    "    \n",
    "    infinity_cost = float('inf')\n",
    "    min_cost_dp = np.full((num_available_vehicles + 1, num_customers + 1), infinity_cost)\n",
    "    # Backtrack table: route_split_points[vehicles_used][customers_covered] = where to split routes\n",
    "    route_split_points = np.full((num_available_vehicles + 1, num_customers + 1), -1, dtype=int)\n",
    "    min_cost_dp[0][0] = 0  # Base case: 0 vehicles serve 0 customers with 0 cost\n",
    "   \n",
    "    for vehicles_used in range(1, num_available_vehicles + 1):\n",
    "        \n",
    "        for customers_covered in range(0, num_customers + 1):\n",
    "            \n",
    "            for route_start_idx in range(0, customers_covered + 1):\n",
    "                \n",
    "            \n",
    "                if customers_covered == route_start_idx:\n",
    "                    # Empty route: vehicle stays at depot, contributes 0 to total distance\n",
    "                    internal_route_distance = 0.0\n",
    "                    total_route_cost = 0.0 \n",
    "                else:\n",
    "                    # Non-empty route: vehicle serves customers route_start_idx to customers_covered-1\n",
    "                    if customers_covered <= route_start_idx:\n",
    "                        internal_route_distance = 0.0\n",
    "                    else:\n",
    "                        # Sum of distances between consecutive customers in this route segment\n",
    "                        internal_route_distance = (prefix_distance_sums[customers_covered-1] - \n",
    "                                                 prefix_distance_sums[route_start_idx])\n",
    "                    \n",
    "                    first_customer_idx = individual_permutation[route_start_idx]\n",
    "                    last_customer_idx = individual_permutation[customers_covered-1]\n",
    "                    \n",
    "                    if depot_to_customer_distances is not None:\n",
    "                        # Total route cost: depotâ†’first + internal + lastâ†’depot\n",
    "                        total_route_cost = (depot_to_customer_distances[first_customer_idx] + \n",
    "                                          internal_route_distance + \n",
    "                                          depot_to_customer_distances[last_customer_idx])\n",
    "                    else:\n",
    "                        total_route_cost = (distance(depot, locations[first_customer_idx]) + \n",
    "                                          internal_route_distance + \n",
    "                                          distance(locations[last_customer_idx], depot))\n",
    "                \n",
    "                # Check if using this route improves the solution\n",
    "                new_total_cost = min_cost_dp[vehicles_used-1][route_start_idx] + total_route_cost\n",
    "                if new_total_cost < min_cost_dp[vehicles_used][customers_covered]:\n",
    "                    min_cost_dp[vehicles_used][customers_covered] = new_total_cost\n",
    "                    route_split_points[vehicles_used][customers_covered] = route_start_idx\n",
    "    \n",
    "    optimal_total_distance = min_cost_dp[num_available_vehicles][num_customers]\n",
    "    if optimal_total_distance == infinity_cost:\n",
    "        return infinity_cost, []\n",
    "    \n",
    "    # Reconstruct optimal routes from backtrack table (excluding empty routes from output)\n",
    "    reconstructed_routes = []\n",
    "    current_vehicles = num_available_vehicles\n",
    "    current_customers = num_customers\n",
    "    while current_customers > 0 and current_vehicles >= 0:\n",
    "        split_point = route_split_points[current_vehicles][current_customers]\n",
    "        if split_point == -1:\n",
    "            return infinity_cost, []  # Should not happen with fixed DP\n",
    "        if current_customers - split_point > 0:  # Only include non-empty routes\n",
    "            route_segment = individual_permutation[split_point:current_customers]\n",
    "            reconstructed_routes.append(route_segment)\n",
    "        current_customers = split_point\n",
    "        current_vehicles -= 1\n",
    "    reconstructed_routes.reverse()\n",
    "    return optimal_total_distance, reconstructed_routes\n",
    "\n",
    "def fitness(individual_permutation):\n",
    "    actual_total_distance, _ = get_vrp_cost_and_routes(individual_permutation, cars)\n",
    "    return compute_normalized_solution_quality(actual_total_distance)\n",
    "\n",
    "def create_individual():\n",
    "    return random.sample(range(len(locations)), len(locations))\n",
    "\n",
    "\n",
    "def crossover(parent1, parent2, crossover_probability):\n",
    "    \"\"\"Optimized PMX with faster mapping resolution\"\"\"\n",
    "    if random.random() >= crossover_probability:\n",
    "        return parent1[:] if random.random() < 0.5 else parent2[:]\n",
    "    \n",
    "    size = len(parent1)\n",
    "    cx_point1 = random.randint(0, size - 1)\n",
    "    cx_point2 = random.randint(0, size - 1)\n",
    "    if cx_point1 > cx_point2:\n",
    "        cx_point1, cx_point2 = cx_point2, cx_point1\n",
    "    \n",
    "    child = [-1] * size\n",
    "    child[cx_point1:cx_point2+1] = parent1[cx_point1:cx_point2+1]\n",
    "    \n",
    "    # Faster mapping with conflict resolution\n",
    "    used = set(child[cx_point1:cx_point2+1])  # Track used values\n",
    "    mapping = {}\n",
    "    \n",
    "    # Build mapping only for conflicts\n",
    "    for i in range(cx_point1, cx_point2 + 1):\n",
    "        if parent2[i] not in used:\n",
    "            mapping[parent2[i]] = parent1[i]\n",
    "    \n",
    "    # Fill positions with optimized lookup\n",
    "    for i in range(size):\n",
    "        if child[i] == -1:\n",
    "            candidate = parent2[i]\n",
    "            # Resolve mapping chain with cycle detection\n",
    "            while candidate in mapping and candidate in used:\n",
    "                candidate = mapping[candidate]\n",
    "            child[i] = candidate\n",
    "            used.add(candidate)\n",
    "    \n",
    "    return child\n",
    "\n",
    "\n",
    "def mutate(individual,mutation_rate):\n",
    "    if random.random() < mutation_rate:\n",
    "        i, j = random.sample(range(len(individual)), 2)\n",
    "        individual[i], individual[j] = individual[j], individual[i]\n",
    "    return individual\n",
    "\n",
    "def tournament_selection(fitnesses,k=3):\n",
    "    indices = random.sample(range(len(fitnesses)), k)\n",
    "    return max(indices, key=lambda i: fitnesses[i])\n",
    "\n",
    "def visualize_individual(individual, fitness_value):\n",
    "    total_dist, routes = get_vrp_cost_and_routes(individual, cars)\n",
    "    route_str = \"\\n\".join([f\"Vehicle {r+1}: depot -> \" + \" -> \".join(map(str, route)) + \" -> depot\" for r, route in enumerate(routes)])\n",
    "    return f\"{route_str}\\nTotal Distance: {total_dist:.2f} Fitness: {fitness_value:.6f}\"\n",
    "\n",
    "def genetic_algorithm_with_tracking(generations, population_size, mutation_rate, crossover_probability): #added: all parameters\n",
    "    \"\"\"\n",
    "    FIX: Genetic algorithm using normalized solution quality as unified evaluation metric\n",
    "    Returns evolution series of best normalized quality scores per generation\n",
    "    \"\"\"\n",
    "    current_population = [create_individual() for _ in range(population_size)]\n",
    "    best_solution_quality_per_generation = []\n",
    "    num_elite_individuals = 1\n",
    "    \n",
    "    current_population_fitness_scores = [fitness(individual) for individual in current_population]\n",
    "    \n",
    "    for generation_number in range(generations):\n",
    "        # Track best solution quality (highest normalized score = best performance)\n",
    "        best_quality_this_generation = max(current_population_fitness_scores)\n",
    "        best_solution_quality_per_generation.append(best_quality_this_generation)\n",
    "        \n",
    "        # Elite selection based on normalized solution quality\n",
    "        fitness_ranked_indices = sorted(range(len(current_population)), \n",
    "                                       key=lambda idx: current_population_fitness_scores[idx], \n",
    "                                       reverse=True)\n",
    "        elite_individuals = [current_population[idx][:] for idx in fitness_ranked_indices[:num_elite_individuals]]\n",
    "        \n",
    "        new_generation_population = []\n",
    "        # Generate offspring using tournament selection on normalized quality scores\n",
    "        num_breeding_pairs = (population_size - num_elite_individuals) // 2\n",
    "        for breeding_iteration in range(num_breeding_pairs):\n",
    "            #Tournament selection operates on normalized solution quality values\n",
    "            parent1_index = tournament_selection(current_population_fitness_scores, k=3)\n",
    "            parent2_index = tournament_selection(current_population_fitness_scores, k=3)\n",
    "            parent1_individual = current_population[parent1_index]\n",
    "            parent2_individual = current_population[parent2_index]\n",
    "            \n",
    "            offspring1 = crossover(parent1_individual, parent2_individual, crossover_probability) #added: parameter\n",
    "            offspring2 = crossover(parent2_individual, parent1_individual, crossover_probability) #added: parameter\n",
    "            new_generation_population.extend([mutate(offspring1, mutation_rate), mutate(offspring2, mutation_rate)]) #added: parameters\n",
    "        \n",
    "        # Preserve elite individuals in new generation\n",
    "        new_generation_population.extend(elite_individuals)\n",
    "        \n",
    "        # Handle population size consistency for odd numbers\n",
    "        while len(new_generation_population) < population_size:\n",
    "            new_generation_population.append(elite_individuals[0][:])\n",
    "        \n",
    "        current_population = new_generation_population[:population_size]\n",
    "        #Recompute normalized solution quality scores for new population\n",
    "        current_population_fitness_scores = [fitness(individual) for individual in current_population]\n",
    "    \n",
    "    #Return time series of best normalized solution quality values\n",
    "    return best_solution_quality_per_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒ½ðŸŒ½ FAST TEST ðŸŒ½ðŸŒ½\n",
    "# Cleaned single cell: group summary + readable group plots (no duplicates)\n",
    "\n",
    "if 'parameter_sets' not in globals():\n",
    "    parameter_sets = [\n",
    "        {\"name\":\"Conservative\",\"generations\":30,\"population_size\":10,\"mutation_rate\":0.1,\"crossover_probability\":0.6},\n",
    "        {\"name\":\"Balanced\",\"generations\":50,\"population_size\":15,\"mutation_rate\":0.3,\"crossover_probability\":0.8},\n",
    "        {\"name\":\"Aggressive\",\"generations\":70,\"population_size\":20,\"mutation_rate\":0.5,\"crossover_probability\":0.9},\n",
    "    ]\n",
    "\n",
    "# Initialize variables to avoid \"not defined\" errors\n",
    "if 'experimental_results' not in globals():\n",
    "    experimental_results = {}\n",
    "if 'timing_performance_results' not in globals():\n",
    "    timing_performance_results = {}\n",
    "\n",
    "# FIX: Auto-run quick experiment if no data exists\n",
    "if (not isinstance(experimental_results, dict) or \n",
    "    len(experimental_results) == 0):\n",
    "    \n",
    "    print(\"ðŸŒ½ No experimental data found. Running QUICK TEST experiment...\")\n",
    "    \n",
    "    # Quick test parameters (much faster)\n",
    "    quick_parameter_sets = [\n",
    "        {\"name\":\"Conservative\",\"generations\":5,\"population_size\":5,\"mutation_rate\":0.1,\"crossover_probability\":0.6},\n",
    "        {\"name\":\"Balanced\",\"generations\":7,\"population_size\":6,\"mutation_rate\":0.3,\"crossover_probability\":0.8},\n",
    "        {\"name\":\"Aggressive\",\"generations\":4,\"population_size\":4,\"mutation_rate\":0.5,\"crossover_probability\":0.9},\n",
    "    ]\n",
    "    \n",
    "    # Quick scenarios (smaller problems) - FIX: Include all 6 scenarios for complete testing\n",
    "    quick_scenarios = [\n",
    "        {\"id\":\"small-1\", \"num_vehicles\":2,  \"num_customers\":6, \"customers\": generate_locations(6, seed=42)},\n",
    "        {\"id\":\"small-2\", \"num_vehicles\":3,  \"num_customers\":8, \"customers\": generate_locations(8, seed=43)},\n",
    "        {\"id\":\"medium-1\",\"num_vehicles\":3, \"num_customers\":10, \"customers\": generate_locations(10, seed=44)},\n",
    "        {\"id\":\"medium-2\",\"num_vehicles\":4, \"num_customers\":12, \"customers\": generate_locations(12, seed=45)},\n",
    "        {\"id\":\"large-1\", \"num_vehicles\":4, \"num_customers\":14, \"customers\": generate_locations(14, seed=46)},\n",
    "        {\"id\":\"large-2\", \"num_vehicles\":5, \"num_customers\":16, \"customers\": generate_locations(16, seed=47)},\n",
    "    ]\n",
    "    \n",
    "    experimental_results = {}\n",
    "    timing_performance_results = {}\n",
    "    num_independent_trials = 2  # Fewer trials for speed\n",
    "    \n",
    "    for parameter_configuration in quick_parameter_sets:\n",
    "        max_generations = parameter_configuration[\"generations\"]\n",
    "        population_size = parameter_configuration[\"population_size\"]\n",
    "        mutation_rate = parameter_configuration[\"mutation_rate\"]\n",
    "        crossover_probability = parameter_configuration[\"crossover_probability\"]\n",
    "        \n",
    "        print(f\"\\n=== QUICK TEST: {parameter_configuration['name']} ===\")\n",
    "        experimental_results[parameter_configuration['name']] = {}\n",
    "        timing_performance_results[parameter_configuration['name']] = {}\n",
    "        \n",
    "        for scenario_config in quick_scenarios:\n",
    "            scenario_id = scenario_config['id']\n",
    "            num_vehicles = scenario_config['num_vehicles'] \n",
    "            num_customers = scenario_config['num_customers']\n",
    "            \n",
    "            print(f\"Running {scenario_id} (V:{num_vehicles}, C:{num_customers})...\", end=\"\", flush=True)\n",
    "            applay_scenario(scenario_config)\n",
    "            \n",
    "            trial_solution_quality_series = []\n",
    "            final_solution_quality_scores = []\n",
    "            trial_execution_times = []\n",
    "            \n",
    "            for trial_number in range(num_independent_trials):\n",
    "                trial_start_time = time.time()\n",
    "                best_quality_evolution = genetic_algorithm_with_tracking(\n",
    "                    generations=max_generations,\n",
    "                    population_size=population_size,\n",
    "                    mutation_rate=mutation_rate,\n",
    "                    crossover_probability=crossover_probability\n",
    "                )\n",
    "                trial_end_time = time.time()\n",
    "                trial_duration = trial_end_time - trial_start_time\n",
    "                \n",
    "                trial_solution_quality_series.append(best_quality_evolution)\n",
    "                final_quality_score = best_quality_evolution[-1] if best_quality_evolution else 0.0\n",
    "                final_solution_quality_scores.append(final_quality_score)\n",
    "                trial_execution_times.append(trial_duration)\n",
    "            \n",
    "            quality_scores_array = np.array(final_solution_quality_scores)\n",
    "            execution_times_array = np.array(trial_execution_times)\n",
    "            \n",
    "            experimental_results[parameter_configuration['name']][scenario_id] = {\n",
    "                'trials': trial_solution_quality_series,\n",
    "                'final_fitnesses': final_solution_quality_scores,\n",
    "                'best': float(np.max(quality_scores_array)),\n",
    "                'worst': float(np.min(quality_scores_array)),\n",
    "                'avg': float(np.mean(quality_scores_array)),\n",
    "                'std': float(np.std(quality_scores_array))\n",
    "            }\n",
    "            \n",
    "            timing_performance_results[parameter_configuration['name']][scenario_id] = {\n",
    "                'avg_time': float(np.mean(execution_times_array)),\n",
    "                'std_time': float(np.std(execution_times_array)),\n",
    "                'trial_times': trial_execution_times\n",
    "            }\n",
    "            \n",
    "            print(f\" done ({np.mean(execution_times_array):.1f}s)\")\n",
    "    \n",
    "    print(\"ðŸŒ½ Quick test experiment completed!\")\n",
    "\n",
    "else:\n",
    "    print(\"ðŸŒ½ Using existing experimental data...\")\n",
    "\n",
    "# Groups and visuals\n",
    "groups = {\n",
    "    \"Small\": [\"small-1\", \"small-2\"],\n",
    "    \"Medium\": [\"medium-1\", \"medium-2\"],\n",
    "    \"Large\": [\"large-1\", \"large-2\"],\n",
    "}\n",
    "\n",
    "param_colors = {'Conservative': '#1f77b4', 'Balanced': '#ff7f0e', 'Aggressive': '#2ca02c'}\n",
    "param_linestyles = {'Conservative': '-', 'Balanced': '--', 'Aggressive': ':'}\n",
    "param_markers = {'Conservative': 'o', 'Balanced': 's', 'Aggressive': '^'}\n",
    "\n",
    "# Debug: Check what data is actually available\n",
    "print(\"\\n=== AVAILABLE DATA DEBUG ===\")\n",
    "for param_name in parameter_sets:\n",
    "    pname = param_name['name']\n",
    "    if pname in experimental_results:\n",
    "        scenarios = list(experimental_results[pname].keys())\n",
    "        print(f\"{pname}: {scenarios}\")\n",
    "    else:\n",
    "        print(f\"{pname}: NO DATA\")\n",
    "\n",
    "# Print nicely aligned summary by group (mirrors run output)\n",
    "print(\"\\n=== SUMMARY BY GROUP ===\")\n",
    "header = f\"{'Group':8} {'ParamSet':12} {'Scenario':10} {'Best':>12} {'Avg':>12} {'Worst':>12} {'Time(s)':>9}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for gname, sids in groups.items():\n",
    "    for p in parameter_sets:\n",
    "        pname = p['name']\n",
    "        for sid in sids:\n",
    "            vals = experimental_results.get(pname, {}).get(sid)\n",
    "            if vals:\n",
    "                t = timing_performance_results.get(pname, {}).get(sid, {}).get('avg_time', 0.0)\n",
    "                print(f\"{gname:8} {pname:12} {sid:10} {vals['best']:12.6f} {vals['avg']:12.6f} {vals['worst']:12.6f} {t:9.2f}\")\n",
    "            else:\n",
    "                print(f\"{gname:8} {pname:12} {sid:10} {'NO DATA':>12} {'NO DATA':>12} {'NO DATA':>12} {'NO DATA':>9}\")\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "# Group-wise plots: mean Â± std across scenarios and trials\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "plt.subplots_adjust(wspace=0.25)\n",
    "\n",
    "for ax, (gname, sids) in zip(axs, groups.items()):\n",
    "    ax.set_title(f\"{gname} scenarios\", fontsize=14)\n",
    "    ax.set_xlabel(\"Generation\", fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.35)\n",
    "\n",
    "    for p in parameter_sets:\n",
    "        pname = p['name']\n",
    "        # Skip parameter sets not in experimental results\n",
    "        if pname not in experimental_results:\n",
    "            continue\n",
    "            \n",
    "        gens = p['generations']\n",
    "        stacked = []\n",
    "        for sid in sids:\n",
    "            entry = experimental_results.get(pname, {}).get(sid)\n",
    "            if not entry:\n",
    "                continue\n",
    "            trials = entry.get('trials', [])\n",
    "            if not trials:\n",
    "                continue\n",
    "            arr = np.array(trials)\n",
    "            if arr.size == 0:\n",
    "                continue\n",
    "            # normalize/truncate/pad to parameter set generations\n",
    "            if arr.shape[1] > gens:\n",
    "                arr = arr[:, :gens]\n",
    "            elif arr.shape[1] < gens:\n",
    "                pad = np.tile(arr[:, -1:], (1, gens - arr.shape[1]))\n",
    "                arr = np.concatenate([arr, pad], axis=1)\n",
    "            stacked.append(arr)\n",
    "\n",
    "        if not stacked:\n",
    "            continue\n",
    "        all_trials = np.vstack(stacked)  # shape (total_trials, gens)\n",
    "        mean_per_gen = np.mean(all_trials, axis=0)\n",
    "        std_per_gen = np.std(all_trials, axis=0)\n",
    "        gens_range = np.arange(1, len(mean_per_gen) + 1)\n",
    "\n",
    "        ax.plot(gens_range, mean_per_gen, label=pname,\n",
    "                color=param_colors.get(pname), linestyle=param_linestyles.get(pname), linewidth=2,\n",
    "                marker=param_markers.get(pname), markersize=4, markevery=max(1, len(gens_range)//10))\n",
    "        ax.fill_between(gens_range, mean_per_gen - std_per_gen, mean_per_gen + std_per_gen,\n",
    "                        color=param_colors.get(pname), alpha=0.15)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax.set_xlim(left=1)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.set_ylabel(\"\")  # shared ylabel set below\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\"{y:.6f}\"))\n",
    "\n",
    "# shared labels/legend/title\n",
    "axs[0].set_ylabel(\"Best Solution Quality (Baseline Improvement Ratio)\", fontsize=12)\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "if handles:\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.99),\n",
    "               ncol=min(len(parameter_sets), 4), fontsize=11)\n",
    "else:\n",
    "    print(\"Warning: No legend handles found - no data to plot!\")\n",
    "fig.suptitle(\"Solution Quality Evolution by Problem Size (mean Â± std across scenarios & trials)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ec6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_experiment(parameter_sets=None, num_trials=3, seed=42):\n",
    "    \"\"\"\n",
    "    Run complete VRP experiment with all parameter sets and scenarios.\n",
    "    Returns experimental_results and timing_performance_results dictionaries.\n",
    "    \"\"\"\n",
    "    if parameter_sets is None:\n",
    "        parameter_sets = [\n",
    "            {\"name\":\"Conservative\",\"generations\":30,\"population_size\":10,\"mutation_rate\":0.1,\"crossover_probability\":0.6},\n",
    "            {\"name\":\"Balanced\",\"generations\":50,\"population_size\":15,\"mutation_rate\":0.3,\"crossover_probability\":0.8},\n",
    "            {\"name\":\"Aggressive\",\"generations\":70,\"population_size\":20,\"mutation_rate\":0.5,\"crossover_probability\":0.9},\n",
    "        ]\n",
    "    \n",
    "    experimental_results = {}\n",
    "    timing_performance_results = {}\n",
    "    scenario_configurations = make_scenarios(seed=seed)\n",
    "    \n",
    "    print(\"ðŸš€ RUNNING FULL EXPERIMENT FOR ASSIGNMENT REPORT\")\n",
    "    print(\"This will take several minutes but provides correct assignment results...\")\n",
    "\n",
    "    for parameter_configuration in parameter_sets:\n",
    "        max_generations = parameter_configuration[\"generations\"]\n",
    "        population_size = parameter_configuration[\"population_size\"]\n",
    "        mutation_rate = parameter_configuration[\"mutation_rate\"]\n",
    "        crossover_probability = parameter_configuration[\"crossover_probability\"]\n",
    "\n",
    "        print(f\"\\n=== PARAMETER SET: {parameter_configuration['name']} ===\")\n",
    "        print(f\"    Generations={max_generations}, Population={population_size}, Mutation={mutation_rate}, Crossover={crossover_probability}\")\n",
    "        \n",
    "        experimental_results[parameter_configuration['name']] = {}\n",
    "        timing_performance_results[parameter_configuration['name']] = {}\n",
    "\n",
    "        for scenario_config in scenario_configurations:\n",
    "            scenario_id = scenario_config['id']\n",
    "            num_vehicles = scenario_config['num_vehicles'] \n",
    "            num_customers = scenario_config['num_customers']\n",
    "            \n",
    "            print(f\"\\nRunning scenario: {scenario_id} (Vehicles: {num_vehicles}, Customers: {num_customers})\")\n",
    "            applay_scenario(scenario_config)\n",
    "\n",
    "            trial_solution_quality_series = []\n",
    "            final_solution_quality_scores = []\n",
    "            trial_execution_times = []\n",
    "\n",
    "            for trial_number in range(num_trials):\n",
    "                print(f\"  Trial {trial_number+1}/{num_trials}...\", end=\"\", flush=True)\n",
    "                trial_start_time = time.time()\n",
    "                \n",
    "                best_quality_evolution = genetic_algorithm_with_tracking(\n",
    "                    generations=max_generations,\n",
    "                    population_size=population_size,\n",
    "                    mutation_rate=mutation_rate,\n",
    "                    crossover_probability=crossover_probability\n",
    "                )\n",
    "                \n",
    "                trial_end_time = time.time()\n",
    "                trial_duration = trial_end_time - trial_start_time\n",
    "                \n",
    "                trial_solution_quality_series.append(best_quality_evolution)\n",
    "                final_quality_score = best_quality_evolution[-1] if best_quality_evolution else 0.0\n",
    "                final_solution_quality_scores.append(final_quality_score)\n",
    "                trial_execution_times.append(trial_duration)\n",
    "                print(f\" completed ({trial_duration:.2f}s)\")\n",
    "\n",
    "            quality_scores_array = np.array(final_solution_quality_scores)\n",
    "            execution_times_array = np.array(trial_execution_times)\n",
    "            \n",
    "            experimental_results[parameter_configuration['name']][scenario_id] = {\n",
    "                'trials': trial_solution_quality_series,\n",
    "                'final_fitnesses': final_solution_quality_scores,\n",
    "                'best': float(np.max(quality_scores_array)),\n",
    "                'worst': float(np.min(quality_scores_array)),\n",
    "                'avg': float(np.mean(quality_scores_array)),\n",
    "                'std': float(np.std(quality_scores_array))\n",
    "            }\n",
    "            \n",
    "            timing_performance_results[parameter_configuration['name']][scenario_id] = {\n",
    "                'avg_time': float(np.mean(execution_times_array)),\n",
    "                'std_time': float(np.std(execution_times_array)),\n",
    "                'trial_times': trial_execution_times\n",
    "            }\n",
    "\n",
    "            print(f\"  Best quality: {experimental_results[parameter_configuration['name']][scenario_id]['best']:.6f}  \" +\n",
    "                  f\"Avg quality: {experimental_results[parameter_configuration['name']][scenario_id]['avg']:.6f} Â± {experimental_results[parameter_configuration['name']][scenario_id]['std']:.6f}  \" +\n",
    "                  f\"Time: {timing_performance_results[parameter_configuration['name']][scenario_id]['avg_time']:.2f}s Â± {timing_performance_results[parameter_configuration['name']][scenario_id]['std_time']:.2f}s\")\n",
    "\n",
    "    print(\"\\nâœ… FULL EXPERIMENT COMPLETED!\")\n",
    "    return experimental_results, timing_performance_results\n",
    "\n",
    "def print_fitness_summary_table(experimental_results, parameter_sets):\n",
    "    \"\"\"Print clean fitness scores summary table for report analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*85)\n",
    "    print(\"FITNESS SCORES SUMMARY TABLE (for Report Analysis)\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"{'Group':8} {'ParamSet':12} {'Scenario':10} {'Best':>12} {'Avg':>12} {'Worst':>12} {'Std Dev':>12}\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    scenario_groups = [\n",
    "        ('Small', ['small-1', 'small-2']),\n",
    "        ('Medium', ['medium-1', 'medium-2']), \n",
    "        ('Large', ['large-1', 'large-2'])\n",
    "    ]\n",
    "\n",
    "    for group_name, scenarios in scenario_groups:\n",
    "        for param_config in parameter_sets:\n",
    "            param_name = param_config['name']\n",
    "            for scenario_id in scenarios:\n",
    "                if (param_name in experimental_results and \n",
    "                    scenario_id in experimental_results[param_name]):\n",
    "                    stats = experimental_results[param_name][scenario_id]\n",
    "                    print(f\"{group_name:8} {param_name:12} {scenario_id:10} \"\n",
    "                          f\"{stats['best']:12.6f} {stats['avg']:12.6f} \"\n",
    "                          f\"{stats['worst']:12.6f} {stats['std']:12.6f}\")\n",
    "                else:\n",
    "                    print(f\"{group_name:8} {param_name:12} {scenario_id:10} \"\n",
    "                          f\"{'NO DATA':>12} {'NO DATA':>12} {'NO DATA':>12} {'NO DATA':>12}\")\n",
    "        print(\"-\" * 85)\n",
    "\n",
    "def plot_experimental_results(experimental_results, timing_performance_results, parameter_sets):\n",
    "    \"\"\"Create comprehensive visualization of experimental results.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Define colors and styles for consistency\n",
    "    param_colors = {'Conservative': '#1f77b4', 'Balanced': '#ff7f0e', 'Aggressive': '#2ca02c'}\n",
    "    param_linestyles = {'Conservative': '-', 'Balanced': '--', 'Aggressive': ':'}\n",
    "    param_markers = {'Conservative': 'o', 'Balanced': 's', 'Aggressive': '^'}\n",
    "\n",
    "    # Helper function to plot scenario group\n",
    "    def plot_scenario_group(ax, title, scenarios):\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        for param_config in parameter_sets:\n",
    "            param_name = param_config['name']\n",
    "            if param_name in experimental_results:\n",
    "                for scenario_id in scenarios:\n",
    "                    if scenario_id in experimental_results[param_name]:\n",
    "                        trial_series = experimental_results[param_name][scenario_id]['trials']\n",
    "                        if trial_series:\n",
    "                            trial_array = np.array(trial_series)\n",
    "                            mean_quality = np.mean(trial_array, axis=0)\n",
    "                            std_quality = np.std(trial_array, axis=0)\n",
    "                            gens_range = range(1, len(mean_quality) + 1)\n",
    "                            \n",
    "                            label = f\"{scenario_id} - {param_name}\"\n",
    "                            ax.plot(gens_range, mean_quality, \n",
    "                                    color=param_colors[param_name], \n",
    "                                    linestyle=param_linestyles[param_name],\n",
    "                                    marker=param_markers[param_name],\n",
    "                                    markersize=3, markevery=max(1, len(gens_range)//8),\n",
    "                                    label=label, linewidth=2, alpha=0.8)\n",
    "                            ax.fill_between(gens_range, mean_quality - std_quality, mean_quality + std_quality,\n",
    "                                            color=param_colors[param_name], alpha=0.1)\n",
    "\n",
    "        ax.set_xlabel('Generation', fontsize=11)\n",
    "        ax.set_ylabel('Solution Quality', fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # Plot scenario groups\n",
    "    plot_scenario_group(ax1, 'Small Scenarios (12-18 customers)', ['small-1', 'small-2'])\n",
    "    plot_scenario_group(ax2, 'Medium Scenarios (20-28 customers)', ['medium-1', 'medium-2'])\n",
    "    plot_scenario_group(ax3, 'Large Scenarios (30-40 customers)', ['large-1', 'large-2'])\n",
    "\n",
    "    # Subplot 4: Execution Time Comparison\n",
    "    ax4.set_title('Average Execution Time by Scenario & Parameters', fontsize=14, fontweight='bold')\n",
    "    scenario_groups_dict = {'Small': ['small-1', 'small-2'], 'Medium': ['medium-1', 'medium-2'], 'Large': ['large-1', 'large-2']}\n",
    "    x_pos = np.arange(len(scenario_groups_dict))\n",
    "    width = 0.25\n",
    "\n",
    "    for i, param_config in enumerate(parameter_sets):\n",
    "        param_name = param_config['name']\n",
    "        avg_times = []\n",
    "        std_times = []\n",
    "        \n",
    "        for group_name, scenarios in scenario_groups_dict.items():\n",
    "            group_times = []\n",
    "            for scenario_id in scenarios:\n",
    "                if (param_name in timing_performance_results and \n",
    "                    scenario_id in timing_performance_results[param_name]):\n",
    "                    group_times.append(timing_performance_results[param_name][scenario_id]['avg_time'])\n",
    "            \n",
    "            if group_times:\n",
    "                avg_times.append(np.mean(group_times))\n",
    "                std_times.append(np.std(group_times))\n",
    "            else:\n",
    "                avg_times.append(0)\n",
    "                std_times.append(0)\n",
    "        \n",
    "        ax4.bar(x_pos + i * width, avg_times, width, \n",
    "               yerr=std_times, capsize=5,\n",
    "               label=param_name, color=param_colors[param_name], alpha=0.8)\n",
    "\n",
    "    ax4.set_xlabel('Problem Size Category', fontsize=11)\n",
    "    ax4.set_ylabel('Average Execution Time (seconds)', fontsize=11)\n",
    "    ax4.set_xticks(x_pos + width)\n",
    "    ax4.set_xticklabels(scenario_groups_dict.keys())\n",
    "    ax4.legend(fontsize=10)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Overall title\n",
    "    fig.suptitle('VRP Genetic Algorithm Performance Analysis\\n(Solution Quality Evolution & Timing)', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "def print_timing_analysis_table(timing_performance_results, parameter_sets):\n",
    "    \"\"\"Print detailed timing analysis table for report.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED TIMING ANALYSIS TABLE (for Report)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Parameter Set':15} {'Problem Size':12} {'Scenario':10} {'Avg Time(s)':12} {'Std Time(s)':12} {'Trials':8}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for param_config in parameter_sets:\n",
    "        param_name = param_config['name']\n",
    "        if param_name in timing_performance_results:\n",
    "            for group_name, scenarios in [('Small', ['small-1', 'small-2']), \n",
    "                                         ('Medium', ['medium-1', 'medium-2']), \n",
    "                                         ('Large', ['large-1', 'large-2'])]:\n",
    "                for scenario_id in scenarios:\n",
    "                    if scenario_id in timing_performance_results[param_name]:\n",
    "                        stats = timing_performance_results[param_name][scenario_id]\n",
    "                        print(f\"{param_name:15} {group_name:12} {scenario_id:10} {stats['avg_time']:12.2f} {stats['std_time']:12.2f} {len(stats['trial_times']):8}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"\\nSUMMARY STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    for param_config in parameter_sets:\n",
    "        param_name = param_config['name']\n",
    "        if param_name in timing_performance_results:\n",
    "            all_times = []\n",
    "            for scenario_times in timing_performance_results[param_name].values():\n",
    "                all_times.extend(scenario_times['trial_times'])\n",
    "            \n",
    "            if all_times:\n",
    "                print(f\"{param_name:15}: Avg={np.mean(all_times):6.2f}s, Min={np.min(all_times):6.2f}s, Max={np.max(all_times):6.2f}s\")\n",
    "\n",
    "# MAIN EXECUTION - Now much cleaner!\n",
    "if __name__ == \"__main__\" or True:  # Run in Jupyter\n",
    "    # Define parameter sets\n",
    "    parameter_sets = [\n",
    "        {\"name\":\"Conservative\",\"generations\":30,\"population_size\":10,\"mutation_rate\":0.1,\"crossover_probability\":0.6},\n",
    "        {\"name\":\"Balanced\",\"generations\":50,\"population_size\":15,\"mutation_rate\":0.3,\"crossover_probability\":0.8},\n",
    "        {\"name\":\"Aggressive\",\"generations\":70,\"population_size\":20,\"mutation_rate\":0.5,\"crossover_probability\":0.9},\n",
    "    ]\n",
    "    \n",
    "    # Run full experiment\n",
    "    experimental_results, timing_performance_results = run_full_experiment(\n",
    "        parameter_sets=parameter_sets, \n",
    "        num_trials=3, \n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Generate all reports and visualizations\n",
    "    print_fitness_summary_table(experimental_results, parameter_sets)\n",
    "    plot_experimental_results(experimental_results, timing_performance_results, parameter_sets)\n",
    "    print_timing_analysis_table(timing_performance_results, parameter_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in3050",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
